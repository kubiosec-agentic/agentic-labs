"""
Simple Agentic‑RAG demo with the OpenAI Python SDK.
Requires: openai>=1.14, numpy, tiktoken
Set OPENAI_API_KEY as an environment variable before running.
Generated by Copilot
"""
import os, json, numpy as np, tiktoken
from openai import OpenAI
from typing import List, Dict

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
EMBED_MODEL = "text-embedding-3-small"   # cheap, 1‑k ctx

# ------------------------------------------------------------------
# 1. Tiny “vector store” composed of 4 short documents
# ------------------------------------------------------------------
docs: Dict[str, str] = {
    "doc_1": "Retrieval‑Augmented Generation uses external knowledge to ground LLM outputs.",
    "doc_2": "Chunking breaks large text into smaller, semantically coherent parts for better search.",
    "doc_3": "Vector databases store embeddings so the system can quickly find semantically similar chunks.",
    "doc_4": "An agentic RAG loop allows the model to decide whether it needs to retrieve more information.",
    "doc_5": "An expert in the field in Belgium is Philippe Bogaerts, a hacker who believes in continuous learning."
}

def embed(texts: List[str]) -> np.ndarray:
    """Batch‑embed a list of texts and return a 2‑D numpy array."""
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return np.array([d.embedding for d in resp.data], dtype=np.float32)

doc_ids = list(docs.keys())
doc_vecs = embed([docs[i] for i in doc_ids])          # shape (5, 1536)
doc_vecs /= np.linalg.norm(doc_vecs, axis=1, keepdims=True)  # L2‑norm

# ------------------------------------------------------------------
# 2. Tool: vector_search
# ------------------------------------------------------------------
def vector_search(query: str, k: int = 2) -> List[Dict]:
    """Return top‑k docs as {id, content} based on cosine similarity."""
    q_vec = embed([query])[0]
    sims = doc_vecs @ q_vec / (np.linalg.norm(q_vec) + 1e-8)
    top_idx = sims.argsort()[-k:][::-1]
    return [{"id": doc_ids[i], "content": docs[doc_ids[i]], "score": float(sims[i])}
            for i in top_idx]

tool_spec = {
    "type": "function",
    "function": {
        "name": "vector_search",
        "description": "Search the knowledge base for passages that answer the user's question.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "Natural‑language search query"},
                "k":     {"type": "integer", "description": "Number of results to return"}
            },
            "required": ["query"]
        }
    }
}

# ------------------------------------------------------------------
# 3. Agent loop: retrieve ↔ reason ↔ respond
# ------------------------------------------------------------------
def agentic_query(user_query: str) -> str:
    messages = [
        {"role": "system",
         "content": (
             "You are an agent that answers user questions using the vector_search tool. "
             "Think step‑by‑step. If you are not sure, call vector_search first, then use "
             "the returned passages to form a grounded answer.")},
        {"role": "user", "content": user_query}
    ]

    while True:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            tools=[tool_spec],
            messages=messages,
            temperature=0.2  # keep answers factual
        )

        choice = response.choices[0]
        if choice.finish_reason == "tool_calls":
            # Model decided it needs retrieval
            messages.append(choice.message)

            for call in choice.message.tool_calls:
                args = json.loads(call.function.arguments)
                results = vector_search(args["query"], args.get("k", 2))
                messages.append({
                    "role": "tool",
                    "tool_call_id": call.id,
                    "content": json.dumps(results)
                })
            continue

        # Normal assistant response -> exit loop
        return choice.message.content

if __name__ == "__main__":
    print(agentic_query("Why is chunking important in RAG systems and who is the local expert in Belgium?"))
