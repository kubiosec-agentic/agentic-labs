from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, ToolMessage
from langchain_core.tools import tool
from pydantic import BaseModel
import subprocess
import os

# Generated by Copilot

# Define a tool with structured input
class CliCommandInput(BaseModel):
    command: str

@tool("run_cli_command", args_schema=CliCommandInput)
def run_cli_command(command: str) -> str:
    """Execute a CLI command and return the output. Only safe commands are allowed."""
    
    # List of safe commands to prevent security issues
    safe_commands = [
        "ls", "pwd", "whoami", "date", "uptime", "df -h", 
        "ps aux", "top -l 1", "uname -a", "which", "echo"
    ]
    
    # Extract the base command (first word)
    base_command = command.strip().split()[0]
    
    # Check if it's a safe command or starts with a safe command
    is_safe = any(command.startswith(safe_cmd) for safe_cmd in safe_commands)
    
    if not is_safe:
        return f"Command '{command}' is not allowed for security reasons. Allowed commands: {', '.join(safe_commands)}"
    
    try:
        # Execute the command
        result = subprocess.run(
            command, 
            shell=True, 
            capture_output=True, 
            text=True, 
            timeout=10,  # 10 second timeout
            cwd=os.getcwd()  # Run in current working directory
        )
        
        if result.returncode == 0:
            output = result.stdout.strip()
            if not output:
                output = "Command executed successfully (no output)"
            return f"Command: {command}\nOutput:\n{output}"
        else:
            error = result.stderr.strip()
            return f"Command: {command}\nError (return code {result.returncode}):\n{error}"
            
    except subprocess.TimeoutExpired:
        return f"Command '{command}' timed out after 10 seconds"
    except Exception as e:
        return f"Error executing command '{command}': {str(e)}"

# Initialize LLM
llm = ChatOpenAI(model="gpt-4o")

# Bind tools
llm_with_tools = llm.bind_tools([run_cli_command])

# User query asking to list the current directory
query = "Can you list the files and directories in the current directory?"

print(f"User query: {query}")

# Step 1: Let the LLM decide whether to call a tool
tool_call_response = llm_with_tools.invoke(query)
print("\nLLM tool call response:")
print(f"Content: {tool_call_response.content}")
print(f"Tool calls: {tool_call_response.tool_calls}")

# Step 2: Execute the tool if there's a tool call
if tool_call_response.tool_calls:
    tool_call = tool_call_response.tool_calls[0]
    tool_args = tool_call["args"]
    
    print(f"\nExecuting tool with args: {tool_args}")
    tool_result = run_cli_command.invoke(tool_args)
    print(f"\nTool result:\n{tool_result}")

    # Step 3: Feed back tool result to the model
    messages = [
        HumanMessage(content=query),
        tool_call_response,
        ToolMessage(content=tool_result, tool_call_id=tool_call["id"])
    ]

    # Step 4: Get final answer from model
    final_response = llm_with_tools.invoke(messages)
    print(f"\nFinal response from LLM:\n{final_response.content}")
else:
    print("\nNo tool calls were made by the LLM")
    print(f"Direct response: {tool_call_response.content}")
